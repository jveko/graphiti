# Graphiti MCP Server Environment Configuration

# Neo4j Database Configuration
# These settings are used to connect to your Neo4j database
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=demodemo


# OpenAI API Configuration
# Optional when using other LLM providers, but enables cross-encoder reranking for better search results
# OPENAI_API_KEY=your_openai_api_key_here  # Optional: Enables OpenAI cross-encoder reranking (lowest priority)
# MODEL_NAME=gpt-4.1-mini

# Gemini LLM Configuration
# Set this to use Gemini for LLM while keeping other embedders
GOOGLE_API_KEY=your_google_api_key_here
MODEL_NAME=gemini-2.5-flash
SMALL_MODEL_NAME=gemini-2.5-flash-lite

# Optional: Only needed for non-standard OpenAI endpoints
# OPENAI_BASE_URL=https://api.openai.com/v1

# Optional: Group ID for namespacing graph data
# GROUP_ID=my_project

# Optional: Path configuration for Docker
PATH=/root/.local/bin:${PATH}

# Optional: Semaphore limit for concurrent operations
SEMAPHORE_LIMIT=10

# Optional: Memory settings for Neo4j (used in Docker Compose)
NEO4J_server_memory_heap_initial__size=512m
NEO4J_server_memory_heap_max__size=1G
NEO4J_server_memory_pagecache_size=512m

# LLM Configuration
# The LLM provider is selected based on which environment variables are set:
# 1. If GOOGLE_API_KEY is set, Gemini AI Studio will be used
# 2. If AZURE_OPENAI_ENDPOINT is set, Azure OpenAI will be used
# 3. Otherwise, OpenAI will be used (requires OPENAI_API_KEY)

# Gemini AI Studio Configuration (for both LLM and embeddings)
# Optional: Set this to use Google Gemini AI Studio
# GOOGLE_API_KEY=your_google_api_key_here
# MODEL_NAME=gemini-2.5-flash  # Options: gemini-2.5-pro, gemini-2.5-flash, gemini-1.5-pro, gemini-1.5-flash
# SMALL_MODEL_NAME=gemini-2.5-flash-lite-preview-06-17  # Smaller model for simple tasks

# Embedding Configuration
# The embedder is selected based on which environment variables are set:
# 1. If VOYAGE_API_KEY is set, Voyage AI will be used
# 2. If GOOGLE_API_KEY is set, Gemini embeddings will be used
# 3. If AZURE_OPENAI_EMBEDDING_ENDPOINT is set, Azure OpenAI will be used
# 4. Otherwise, OpenAI will be used (requires OPENAI_API_KEY)

# Cross-Encoder Reranking Configuration
# The reranker (cross-encoder) is selected based on which environment variables are set:
# 1. If VOYAGE_API_KEY is set, Voyage reranker will be used (highest priority, best performance)
# 2. If GOOGLE_API_KEY is set, Gemini reranker will be used
# 3. If OPENAI_API_KEY is set, OpenAI reranker will be used
# 4. Otherwise, reranking is disabled (search still works, but less accurate)

# Voyage AI Configuration
# Optional: Set this to use Voyage AI for embeddings AND reranking (highest priority for both)
# VOYAGE_API_KEY=your_voyage_api_key_here
# EMBEDDER_MODEL_NAME=voyage-3  # Embedding options: voyage-3, voyage-3-lite, voyage-finance-2, voyage-multilingual-2
# RERANKER_MODEL_NAME=rerank-2.5  # Reranker options: rerank-2.5, rerank-2.5-lite, rerank-2, rerank-2-lite, rerank-1

# Gemini Embeddings Configuration (uses same GOOGLE_API_KEY as above)
# EMBEDDER_MODEL_NAME=text-embedding-001  # Options: text-embedding-001, gemini-embedding-001

# Azure OpenAI configuration
# Optional: Only needed for Azure OpenAI endpoints
# AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint_here
# AZURE_OPENAI_API_VERSION=2025-01-01-preview
# AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o-gpt-4o-mini-deployment
# AZURE_OPENAI_EMBEDDING_API_VERSION=2023-05-15
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME=text-embedding-3-large-deployment
# AZURE_OPENAI_USE_MANAGED_IDENTITY=false

# OpenAI Embedder Configuration (default)
# EMBEDDER_MODEL_NAME=text-embedding-3-small  # Used when neither Voyage nor Azure is configured
